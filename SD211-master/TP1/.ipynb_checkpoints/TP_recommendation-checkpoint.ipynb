{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Recommendation\n",
    "Loïc Herbelot\n",
    "\n",
    "### Question 1.1 : Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def load_movielens(filename, minidata=False):\n",
    "    \"\"\"\n",
    "    Cette fonction lit le fichier filename de la base de donnees\n",
    "    Movielens, par exemple \n",
    "    filename = '~/datasets/ml-100k/u.data'\n",
    "    Elle retourne \n",
    "    R : une matrice utilisateur-item contenant les scores\n",
    "    mask : une matrice valant 1 si il y a un score et 0 sinon\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, dtype=int)\n",
    "\n",
    "    R = sparse.coo_matrix((data[:, 2], (data[:, 0]-1, data[:, 1]-1)),\n",
    "                          dtype=float)\n",
    "    R = R.toarray()  # not optimized for big data\n",
    "\n",
    "    # code la fonction 1_K\n",
    "    mask = sparse.coo_matrix((np.ones(data[:, 2].shape),\n",
    "                              (data[:, 0]-1, data[:, 1]-1)), dtype=bool )\n",
    "    mask = mask.toarray()  # not optimized for big data\n",
    "\n",
    "    if minidata is True:\n",
    "        R = R[0:100, 0:200].copy()\n",
    "        mask = mask[0:100, 0:200].copy()\n",
    "\n",
    "    return R, mask\n",
    "\n",
    "\n",
    "def objective(P, Q0, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme simplifie.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q0 : une matrice de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = -Q0.T.dot(tmp) + rho*P\n",
    "\n",
    "    return val, grad_P\n",
    "\n",
    "\n",
    "def total_objective(P, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme complet.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q : la variable matricielle de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    grad_Q : le gradient par rapport a Q\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = -Q.T.dot(tmp) + rho*P\n",
    "\n",
    "    grad_Q = -tmp.dot(P.T) + rho*Q   \n",
    "\n",
    "    return val, grad_P, grad_Q\n",
    "\n",
    "\n",
    "def total_objective_vectorized(PQvec, R, mask, rho):\n",
    "    \"\"\"\n",
    "    Vectorisation de la fonction precedente de maniere a ne pas\n",
    "    recoder la fonction gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # reconstruction de P et Q\n",
    "    n_items = R.shape[1]\n",
    "    n_users = R.shape[0]\n",
    "    F = PQvec.shape[0] // (n_items + n_users)\n",
    "    Pvec = PQvec[0:n_items*F]\n",
    "    Qvec = PQvec[n_items*F:]\n",
    "    P = np.reshape(Pvec, (F, n_items))\n",
    "    Q = np.reshape(Qvec, (n_users, F))\n",
    "\n",
    "    val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "    return val, np.concatenate([grad_P.ravel(), grad_Q.ravel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de R: (943, 1682)\n",
      "Taille de mask: (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename = \"ml-100k/u.data\"\n",
    "\n",
    "#sans \"minidata\"\n",
    "R, mask = load_movielens(filename)\n",
    "print(\"Taille de R:\", R.shape)\n",
    "print(\"Taille de mask:\", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de R_mini: (100, 200)\n",
      "Taille de mask_mini: (100, 200)\n"
     ]
    }
   ],
   "source": [
    "#avec \"minidata\"\n",
    "R_mini, mask_mini = load_movielens(filename, minidata=True)\n",
    "print(\"Taille de R_mini:\", R_mini.shape)\n",
    "print(\"Taille de mask_mini:\", mask_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de `minidata` :\n",
    "Comme on peut le voir dans le code source, l'argument `minidata` sert à indiquer si l'on veut une plus petite partie des informations, seulement 100 lignes et 200 colonnes.\n",
    "\n",
    "### Question 1.2 : Informations sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du fichier d'info : \n",
      "943 users\n",
      "1682 items\n",
      "100000 ratings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"ml-100k/u.info\") as f:\n",
    "    print(\"Contenu du fichier d'info : \\n\"+ f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'on a donc :\n",
    " * 943 utilisateurs\n",
    " * 1682 films\n",
    " * Pour un total de 100 000 notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 : Étude de la fonction objectif\n",
    "\n",
    "Ici j'ai tracé la fonction avec deux variables :\n",
    "![Graphe de la fonction objectif avec deux variables](plot_objective_2_var.png)\n",
    "\n",
    "\n",
    "On voit donc que la fonction n'est pas convexe.\n",
    "\n",
    "Le gradient de f est :\n",
    "\n",
    "$\\nabla f(P,Q) = (Q^T (1_K ⋅ (QP-R)) + \\rho P, 1_K ⋅ (QP-R)P^T + \\rho Q)$\n",
    "\n",
    "Le gradient de $f$ n'est pas lipschitzien, car ses dérivées partielles ne sont pas bornées.\n",
    "\n",
    "Exemple en dimension 1 :\n",
    "$f(x,y) = \\frac 1 2 (r- xy)^2 + \\frac \\rho 2 (x^2 + y^2) $\n",
    "\n",
    "$\\frac{df}{dx}(x) = x(y^2 + \\rho) - yr$ et n'est pas bornée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 :\n",
    "\n",
    "La fonction $g$ est convexe.\n",
    "\n",
    "Son gradient vaut : $\\nabla g(P) = -{Q^0}^T(1_K ⋅ (R - Q^0*P)) + \\rho P $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 : Calcul du gradient de $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification sautée.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "# Initialisation des variables :\n",
    "rho = 0.2\n",
    "P = np.random.random((7, 1682))*100\n",
    "\n",
    "# Fonctions utilisées pour la vérification du gradient :\n",
    "\n",
    "# def g(P):\n",
    "#     \"\"\" Renvoie la sortie de la fonction objectif.\n",
    "#     :param P: un vecteur ligne de 7*1682 éléments\n",
    "#     :return : la sortie de la fonction objectif.\"\"\"\n",
    "#     P = P.reshape((7, 1682))\n",
    "#     return objective(P, Q0, R, mask, rho)[0]\n",
    "\n",
    "# def grad_g(P):\n",
    "#     \"\"\"Renvoie le gradient de la fonction objectif au point P.\n",
    "#     :param P: un vecteur ligne de 7*1682 éléments\n",
    "#     :return: le gradient sous forme de vecteur ligne.\"\"\"\n",
    "#     P = P.reshape((7, 1682))\n",
    "#     return objective(P, Q0, R, mask, rho)[1].ravel()\n",
    "\n",
    "# Activer/Désactiver la vérification du gradient :\n",
    "verif = False # Choix de l'utilisateur\n",
    "max_verif = 1\n",
    "if verif :\n",
    "    # Attention, calcul très long :\n",
    "    print(\"Erreur sur le gradient :\")\n",
    "    for i in range(max_verif):\n",
    "        Q0 = np.random.random((943, 7))\n",
    "        P = np.random.random((7, 1682))\n",
    "        print(\"Vérification n°%d/%d :\" % (i+1, max_verif))\n",
    "        print(check_grad(lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[0],\n",
    "                         lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[1].ravel(),\n",
    "                         P.ravel()))\n",
    "else :\n",
    "    print(\"Vérification sautée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 : Gradient à pas constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def gradient(objective, P0, gamma=1, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise une fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param gamma:   Le pas\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "#     print(\"Q0:\",Q0[:3,:3])\n",
    "#     print(\"R:\",R[:3,:3])\n",
    "#     print(\"mask:\",mask[:3,:3])\n",
    "    x = P0\n",
    "    grad = objective(x, Q0, R, mask, rho)[1]\n",
    "    i = 0\n",
    "#     L = rho + np.linalg.norm(Q0.T.dot(Q0), 'fro')\n",
    "#     gamma = 1/L\n",
    "    while i < n_iter and np.linalg.norm(grad, 'fro') > epsilon:\n",
    "        x -= gamma * grad\n",
    "        val, grad = objective(x, Q0, R, mask, rho)\n",
    "        if i%10 == 0 and verbose:\n",
    "            print(\"Iteration n°%5d, g(x) = %E\" % (i, val))\n",
    "        i += 1\n",
    "    if verbose:\n",
    "        print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "    return x, val, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 : Démonstration de la fonction `gradient` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Test du gradient à pas constant\n",
      "Iteration n°    0, g(x) = 2.517931E+08\n",
      "Iteration n°   10, g(x) = 1.487798E+06\n",
      "Iteration n°   20, g(x) = 3.083309E+05\n",
      "Iteration n°   30, g(x) = 2.979317E+05\n",
      "Iteration n°   40, g(x) = 2.978283E+05\n",
      "Valeur maximale du gradient en ce point :  0.00348753462096\n",
      "Minimum trouvé : 2.978272E+05\n",
      "\n",
      "-- Vérifions les valeurs trouvées :\n",
      "Minimum trouvé : 2.978272E+05\n",
      "Valeur maximale du gradient en ce point :  0.00348753462096\n",
      "\n",
      "-- 10 valeurs au hasard :\n",
      "g(P) = 6.183731E+06\n",
      "g(P) = 6.267920E+10\n",
      "g(P) = 6.210209E+06\n",
      "g(P) = 6.789052E+05\n",
      "g(P) = 6.791154E+05\n",
      "g(P) = 6.855632E+05\n",
      "g(P) = 6.789643E+05\n",
      "g(P) = 6.250328E+10\n",
      "g(P) = 6.358311E+10\n",
      "g(P) = 6.234466E+06\n"
     ]
    }
   ],
   "source": [
    "# Trouvons le P qui minimise g :\n",
    "print(\"-- Test du gradient à pas constant\")\n",
    "P_min, val, grad = gradient(objective, np.random.random((7, 1682))*1000, 1, 0.1, n_iter=1000)\n",
    "print(\"Minimum trouvé : %E\" % val)\n",
    "\n",
    "# Vérification :\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "# print(\"Q0:\",Q0[:3,:3])\n",
    "# print(\"R:\",R[:3,:3])\n",
    "# print(\"mask:\",mask[:3,:3])\n",
    "val_v, grad_v = objective(P_min, Q0, R, mask, rho)\n",
    "print(\"\\n-- Vérifions les valeurs trouvées :\")\n",
    "print(\"Minimum trouvé : %E\" % val_v)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad_v))\n",
    "\n",
    "\n",
    "# Essayons 10 valeurs au hasard pour vérifier la pertinence du résultat :\n",
    "print(\"\\n-- 10 valeurs au hasard :\")\n",
    "for i in range(10):\n",
    "    # J'évite de prendre des matrices \"proches\" de 0, \n",
    "    # sinon on ne pourrait pas voir comment se comporte g.\n",
    "    # Je multiplie une matrice \"random\" par 10^[un entier au hasard]\n",
    "    exp = np.random.randint(-1, 5)\n",
    "    P = np.random.random((7, 1682))*10**exp\n",
    "    rand_val = objective(P, Q0, R, mask, rho)[0]\n",
    "    if rand_val < val:\n",
    "        print(\"Nouveau minimum trouvé :\")\n",
    "    print(\"g(P) = %E\" % (rand_val))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 : Recherche linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Gradient linéaire :\n",
      "Iteration n°    0, g(x) = 6.222543E+05\n",
      "Iteration n°  100, g(x) = 2.978951E+05\n",
      "Minimum trouvé : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0374676830037\n",
      "\n",
      "-- Retrouvons les valeurs objectif : \n",
      "Minimum trouvé : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0374676830037\n",
      "\n",
      "-- 10 valeurs au hasard :\n",
      "g(P) = 6.259877E+06\n",
      "g(P) = 6.127981E+06\n",
      "g(P) = 6.787662E+05\n",
      "g(P) = 6.233227E+08\n",
      "g(P) = 6.210673E+10\n",
      "g(P) = 6.276388E+10\n",
      "g(P) = 6.786843E+05\n",
      "g(P) = 6.671965E+05\n",
      "g(P) = 6.224915E+08\n",
      "g(P) = 6.790279E+05\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def lin_gradient(objective, P0, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise la fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "    x = P0\n",
    "    val, grad = objective(x, Q0, R, mask, rho)\n",
    "    i = 0\n",
    "    gamma_values = np.linspace(0.1, 10, 100)\n",
    "#     L = rho + np.linalg.norm(Q0.T.dot(Q0), 'fro')\n",
    "#     gamma = 1/L\n",
    "    while i < n_iter and np.linalg.norm(grad, 'fro') > epsilon:\n",
    "        g = lambda x:objective(x, Q0, R, mask, rho)[0]\n",
    "        id_argmin = np.argmin(lambda gamma: g(x - gamma*grad))\n",
    "        x -= gamma_values[id_argmin] * grad\n",
    "        val, grad = objective(x, Q0, R, mask, rho)\n",
    "        # Affichage des résultats :\n",
    "        if i%100 == 0 and verbose:\n",
    "            print(\"Iteration n°%5d, g(x) = %E\" % (i, val))\n",
    "        i += 1\n",
    "    if verbose:\n",
    "        print(\"Minimum trouvé : %E\" % val)\n",
    "        print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "    return x, val, grad\n",
    "\n",
    "\n",
    "# Trouvons le P qui minimise g :\n",
    "print(\"-- Gradient linéaire :\")\n",
    "P_min, val, grad = lin_gradient(objective, np.random.random((7, 1682)), 1, n_iter=1000)\n",
    "\n",
    "print(\"\\n-- Retrouvons les valeurs objectif : \")\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "val, grad = objective(P_min, Q0, R, mask, rho)\n",
    "print(\"Minimum trouvé : %E\" % val)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "\n",
    "# Essayons 10 valeurs au hasard pour vérifier la pertinence du résultat :\n",
    "print(\"\\n-- 10 valeurs au hasard :\")\n",
    "for i in range(10):\n",
    "    # J'évite de prendre des matrices \"proches\" de 0, \n",
    "    # sinon on ne pourrait pas voir comment se comporte g.\n",
    "    # Je multiplie une matrice \"random\" par 10^[un entier au hasard]\n",
    "    exp = np.random.randint(0, 5)\n",
    "    P = np.random.random((7, 1682))*(10**exp)\n",
    "    rand_val = objective(P, Q0, R, mask, rho)[0]\n",
    "    if rand_val < val:\n",
    "        print(\"Nouveau minimum trouvé :\")\n",
    "    print(\"g(P) = %E\" % (rand_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 3.2 : Gradient conjugué :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 298119.905469\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "Fin de la fonction conj_gradient.\n",
      "\n",
      "\n",
      "-- Vérification :\n",
      "Valeur en P_tilde : 2.981199E+05\n",
      "Valeur maximale du gradient en ce point :  0.714917252935\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def conj_gradient(objective, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise une fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, \n",
    "    et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "    \n",
    "    g       = lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[0]\n",
    "    g_prime = lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[1].ravel()\n",
    "    \n",
    "    approx_P, val, grad = lin_gradient(objective, np.random.random((7, 1682)), 1, n_iter=10, verbose=False)\n",
    "    return fmin_cg(g, P.ravel(), g_prime, gtol=epsilon, maxiter=n_iter, disp=verbose).reshape((7, 1682))\n",
    "\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "P_tilde = conj_gradient(objective,1)\n",
    "print(\"Fin de la fonction conj_gradient.\\n\")\n",
    "print(\"\\n-- Vérification :\")\n",
    "val, grad = objective(P_tilde, Q0, R,mask, rho)\n",
    "print(\"Valeur en P_tilde : %E\" % val)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 3.3 : Comparaison des algos :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode utilisée : 'constant_step'....................\n",
      "Méthode utilisée : 'linear'....................\n",
      "Méthode utilisée : 'conjugate'....................\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "mult = [10.**e for e in np.random.randint(-1, 5, size=20)]\n",
    "rand_mat = [np.random.random((7, 1682)) * mul for mul in mult]\n",
    "\n",
    "methods = {\"constant_step\":{\"scores\":[],\"times\":[], \"func\":gradient},\n",
    "           \"linear\":       {\"scores\":[],\"times\":[], \"func\":lin_gradient},\n",
    "           \"conjugate\":       {\"scores\":[],\"times\":[], \"func\":conj_gradient}}\n",
    "\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "    \n",
    "for meth in (\"constant_step\", \"linear\"):\n",
    "    print(\"Méthode utilisée : '%s'\" % meth, end='')\n",
    "    for mat in rand_mat:\n",
    "        print('.', end='')\n",
    "        t1 = time.clock()\n",
    "        P = methods[meth][\"func\"](objective, mat, verbose=False)[0]\n",
    "        methods[meth][\"times\"].append(time.clock() - t1)\n",
    "        methods[meth][\"scores\"].append(objective(P, Q0, R, mask, rho)[0])\n",
    "    print()\n",
    "    \n",
    "meth = \"conjugate\"\n",
    "print(\"Méthode utilisée : '%s'\" % meth, end='')\n",
    "for mat in rand_mat:\n",
    "    print('.', end='')\n",
    "    t1 = time.clock()\n",
    "    P = methods[meth][\"func\"](objective, verbose=False)\n",
    "    methods[meth][\"times\"].append(time.clock() - t1)\n",
    "    methods[meth][\"scores\"].append(objective(P, Q0, R, mask, rho)[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f34140d6320>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6hSoocLL2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3413fa4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "keys = methods.keys()\n",
    "for meth,symb in zip(keys, \"x^.\"):\n",
    "    plt.scatter(methods[meth][\"times\"], methods[meth][\"scores\"], marker=symb)\n",
    "plt.title(\"Performance des différentes méthodes\")\n",
    "plt.xlabel(\"Temps (s)\")\n",
    "plt.ylabel(\"Minimum trouvé\")\n",
    "plt.legend(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On voit que la méthode de gradient linéaire est assez efficace en terme de temps mais moins régulière que la méthode du gradient conjugué en terme de minimum trouvé.\n",
    "\n",
    "## Partie 4 : Résolution du problème complet\n",
    "\n",
    "### Question 4.1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur sur le gradient :\n",
      "Vérification n°1/1 :\n",
      "0.520710577101\n"
     ]
    }
   ],
   "source": [
    "# Activer/Désactiver la vérification du gradient :\n",
    "verif = False # Choix de l'utilisateur\n",
    "max_verif = 1\n",
    "if verif :\n",
    "    # Attention, calcul très long :\n",
    "    print(\"Erreur sur le gradient :\")\n",
    "    for i in range(max_verif):        \n",
    "        P = np.random.random((7, 1682))\n",
    "        Q = np.random.random((943, 7))\n",
    "        print(\"Vérification n°%d/%d :\" % (i+1, max_verif))\n",
    "        print(check_grad(lambda PQvec:total_objective_vectorized(PQvec, R, mask, rho)[0],\n",
    "                         lambda PQvec:total_objective_vectorized(PQvec, R, mask, rho)[1],\n",
    "                         np.concatenate([P.ravel(), Q.ravel()])))\n",
    "else :\n",
    "    print(\"Vérification sautée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normes des gradients : (776.2721179878007, 776.27210142857029)\n",
      "Iteration n°    0, g(x) = 3.377185E+05\n",
      "Normes des gradients : 1.650769E+05 et 1.927216E+02 \n",
      "Normes des gradients : 2.775697E+10 et 6.707047E+12 \n",
      "Normes des gradients : 3.760966E+30 et 8.591430E+22 \n",
      "Normes des gradients : 1.109916E+56 et 6.165763E+73 \n",
      "Normes des gradients : INF et 6.135219E+134 \n",
      "Normes des gradients : INF et INF \n",
      "Normes des gradients : NAN et NAN \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:75: RuntimeWarning: overflow encountered in square\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:73: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "def total_gradient(total_objective, P, Q, gamma=1, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise une fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param total_objective:       La fonction à minimiser\n",
    "    :param P,Q:      Le point de départ\n",
    "    :param gamma:   Le pas\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point\n",
    "    \"\"\"\n",
    "    \n",
    "    rho = 0.2\n",
    "    val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "    if verbose:\n",
    "        print(\"Normes des gradients :\", (np.linalg.norm(grad_P, 'fro'), np.linalg.norm(grad_Q, 'fro')))\n",
    "    i = 0\n",
    "    # On s'arrête si on a dépassé le nombre maximal d'itérations, \n",
    "    # ou si les 2 gradients sont quasi nuls.\n",
    "    while (i < n_iter and \n",
    "          (np.linalg.norm(grad_P, 'fro') > epsilon or np.linalg.norm(grad_Q, 'fro') > epsilon)):\n",
    "        P -= gamma * grad_P\n",
    "        Q -= gamma * grad_Q\n",
    "        val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "        if i%10 == 0 and verbose:\n",
    "            print(\"Iteration n°%5d, g(x) = %E\" % (i, val))\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print(\"Normes des gradients : %E et %E \" % (np.linalg.norm(grad_P, 'fro'), np.linalg.norm(grad_Q, 'fro')))\n",
    "    return P, Q, val\n",
    "\n",
    "Q, s, P = svds(R, min(7, min(R.shape)))\n",
    "# P, Q = np.zeros(P.shape), np.zeros(Q.shape)\n",
    "P, Q, val = total_gradient(total_objective, P, Q, gamma=1, epsilon=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
