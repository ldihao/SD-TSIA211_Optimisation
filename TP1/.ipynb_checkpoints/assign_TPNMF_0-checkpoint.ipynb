{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab: Nonnegative Matrix Factorization\n",
    "## SD-TSIA 211\n",
    "### Felipe Cerqueira Lyra, João Paulo Bezerra de Araújo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.optimize import check_grad\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Database\n",
    "#### Question 1.1\n",
    "Download and extract the database of faces, collected by AT&T Laboratories Cambridge, on https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.\n",
    "How many images are there in the database? How many pixels are there in each image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "According to the data documentation there are 10 pictures of 40 different subjects, resulting in a total of 400 different pictures. Still acording to the documentation, each picture has 92x112 pixels, or 10304 pixels in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openimage (address):\n",
    "    return np.ravel( plt.imread(address) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 48.,  49.,  45., ...,  47.,  46.,  46.],\n",
       "       [ 60.,  60.,  62., ...,  32.,  34.,  34.],\n",
       "       [ 39.,  44.,  53., ...,  29.,  26.,  29.],\n",
       "       ...,\n",
       "       [125., 119., 124., ...,  36.,  39.,  40.],\n",
       "       [119., 120., 120., ...,  89.,  94.,  85.],\n",
       "       [125., 124., 124., ...,  36.,  35.,  34.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.empty((400,10304), dtype=float)\n",
    "for i in range (1, 41):\n",
    "    folder = \"att_faces/s\" + str(i) + \"/\"\n",
    "    for j in range(1,11):\n",
    "        file = str(j) + \".pgm\"\n",
    "        address = folder + file\n",
    "        image = openimage(address)\n",
    "        M[(i-1)*10 + (j - 1)] = image\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Presentation of the model\n",
    "#### Question 2.1\n",
    "Show that the objective function is not convex. Calculate its gradient. Is the gradient\n",
    "Lipschitz continuous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Non convexity of $f(W, H)$\n",
    "Considering the case $n=p=1$ where x and y are uni-dimensional and the objective function is: $f_X(y,w) = (x - yw)^2$\n",
    "\n",
    "The second derivative of a convex function is always non-negative, and the Hessian matrix must be positive semidefinite. So we can look at the eigenvalues of the Hessian Matrix and see if one of them are negative to show that the function $f_X(y,w)$ is not convex. The gradient of $f_X$ is:\n",
    "\n",
    "\n",
    "$$ \\nabla (f_X) = \\left(\\begin{array}{cc} 2y^2w-2xy\\\\2yw^2-2xw \\end{array} \\right)$$\n",
    "\n",
    "and the Hessian matrix is:\n",
    "\n",
    "\n",
    "$$ \\nabla^2(f_X) = \\left(\\begin{array}{cc} 2y^2 & 4yw-2x \\\\ 4yw-2x & 2w^2\\end{array}\\right)$$\n",
    "\n",
    "if we choose $y = 2$, $w = 3$, and $x = 4$, the eigenvalues are $\\lambda$ where:\n",
    "\n",
    "\n",
    "$$det\\left|\\begin{array}{cc} 8 - \\lambda & 16 \\\\ 16 & 18 - \\lambda \\end{array}\\right| = 0$$\n",
    "\n",
    "so it can be found $\\lambda = -3.763 $, and the Hessian matrix is not positive simidefinite, what implies that $f_X$ is not convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of the Gradient\n",
    "\n",
    "Calculation the gradient of function $f_M(W,H) = \\frac{1}{2np}*||M - WH||_F^2$. The equivalences that will be used are:\n",
    "\n",
    "$$ \\begin{align}\n",
    "||A||^2 &= \\left<A,A\\right> = tr\\left(AA^T\\right) \\\\\n",
    "||A+B||_F^2 &= ||A||_F^2 + ||B||_F^2 + 2\\left<A,B\\right>_F \\\\\n",
    "f_M(W+\\Delta W, H) - f(W,H) &= \\left<\\delta f_M,\\delta W\\right> + ||\\delta W||\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "with $||\\delta W||$ small, so we have:\n",
    "\n",
    "$$\\begin{align}\n",
    "f_M(W,H) &= \\frac{1}{2np}*\\left[\\left(tr(MM^T\\right)+tr\\left(WHH^TW^T\\right)-2*tr\\left(MH^TW^T\\right)\\right]\\\\\n",
    "f_M(W+\\Delta W\\,H) &= \\frac{1}{2np}*\\left[tr\\left(MM^T\\right)+tr\\left((W+\\Delta W)HH^T(W+\\Delta W)^T\\right)-2*tr\\left(MH^T(W+\\Delta W)^T\\right)\\right]\\\\\n",
    "f_M(W+\\Delta W,H) &= \\frac{1}{2np}*\\left[tr\\left(MM^T\\right)+ tr\\left(WHH^TW^T\\right) + tr\\left(WHH^T\\Delta W^T\\right) + tr\\left(\\Delta WHH^TW^T\\right) + tr\\left(\\Delta WHH^T\\Delta W^T\\right)\\\\ \n",
    "-2*tr\\left(MH^TW^T\\right) -2*tr\\left(MH^T\\Delta W^T\\right)\\right]\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\approx \\frac{1}{2np}*\\left[tr\\left(MM^T\\right)+ tr\\left(WHH^TW^T\\right) + tr\\left(WHH^T\\Delta W^T\\right) + tr\\left(\\Delta WHH^TW^T\\right)-2*tr\\left(MH^TW^T\\right) -2*tr\\left(MH^T\\Delta W^T\\right)\\right]$$\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "f_M(W+\\Delta W,H) - f_M(W,H) &= \\frac{1}{np}*\\left[tr\\left(WHH^T\\Delta W^T\\right)-tr\\left(MH^T\\Delta W^T\\right)\\right]\\\\\n",
    "&= \\frac{1}{np}*\\left(\\left<WHH^T,\\Delta W\\right> - \\left<MH^T,\\Delta W\\right>\\right) = \\frac{1}{np}*\\left(\\left<WHH^T-MH^T,\\Delta W\\right>\\right\n",
    ")\\\\\n",
    "&= \\left<\\delta f_M,\\delta W\\right>\\\\\n",
    "\\frac{\\delta f_M}{\\delta W} &= \\frac{1}{np}*(WH - M)H^T\n",
    "\\end{align}$$\n",
    "\n",
    "On the other hand,\n",
    "\n",
    "$$f_M(W,H) = \\frac{1}{2np}*\\left[tr\\left(MM^T\\right)+tr\\left(W(H+\\Delta H)(H+\\Delta H)^TW^T\\right)-2*tr\\left(M(H+\\Delta H)^TW^T\\right)\\right]$$\n",
    "\n",
    "by developing the equation above in a similar way than before, we have\n",
    "\n",
    "$$\\frac{\\delta f_M}{\\delta H} = \\frac{1}{np}*W^T(WH - M)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Lipschitz continuity\n",
    "The gradient matrix of $f_M$ is:\n",
    "$$\\nabla f_M(W,H) = \\left(\\begin{array}{c} \\frac{1}{np}*(WH-M)H^T \\\\ \\frac{1}{np}*W^T(WH-M) \\end{array}\\right)$$\n",
    "\n",
    "To show that the gradient is Lipschitz continuity, all terms on the Hessian matrix must be bounded. Reducing the case to n=p=1 where W,H and M are uni-dimensional, so we have:\n",
    "\n",
    "$$\\nabla f_M(W,H) = \\left(\\begin{array}{cc} H^2 & 2WH-M \\\\ 2WH-M & W^2 \\end{array}\\right)$$\n",
    "\n",
    "Looking to the first element $H^2$, one can\n",
    "see that it doesn't exist $k \\in \\mathbb{R}, k > 0$, where\n",
    "\n",
    "$$H^2 \\leq k$$\n",
    "\n",
    "so the Hessian is not bounded and $\\nabla f_M$ is not Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Find W when H0 is fixed\n",
    "#### Question 3.1\n",
    "We initialize the optimization algorithm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W0, S, H0 = svds(M, 399)\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "H0 = np.maximum(0,(H0.T * np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the advantage of this choice? What would be other possibilities for the initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2\n",
    "Is the objective function g convex? Calculate its gradient. We will admit that the gradient\n",
    "of g is Lipschitz continuous with constant $L0 = ||(H^0)^TH^0||_F$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Convexity of g(W)\n",
    "Starting from a function $z(W) = \\sqrt{g(W)}$, will allow me to use the following properties of the norm operator:\n",
    "$$||A + B|| \\leq ||A|| + ||B||$$\n",
    "$$||\\alpha A|| = |\\alpha|*||A||$$\n",
    "\n",
    "$\\begin{align}\n",
    "z(W)=\\frac{1}{\\sqrt{2np} }||M-WH|| \\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "z(tx + (1-t)y)&\\leq tz(x) + (1-t)z(y)\\\\\n",
    "\\frac{1}{\\sqrt{2np}}||M-(tW_x + (1-t)W_y)H|| &\\leq \\frac{t}{\\sqrt{2np}}||M-W_xH|| + \\frac{1-t}{\\sqrt{2np}}||M-W_yH||\\\\\n",
    "||M-(tW_x + (1-t)W_y)H|| &\\leq t||M-W_xH|| + (1-t)||M-W_yH||\\\\\n",
    "||M-tW_xH - (1-t)W_yH|| &\\leq t||M-W_xH|| + (1-t)||M-W_yH||\\\\\n",
    "||M-tW_xH - (1-t)W_yH|| &\\leq ||tM-tW_xH|| + ||(1-t)M-(1-t)W_yH||\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "||M-tW_xH - (1-t)W_yH|| &\\leq ||tM-tW_xH + (1-t)M-(1-t)W_yH||&\\leq ||tM-tW_xH|| + ||(1-t)M-(1-t)W_yH||\\\\\n",
    "||M-tW_xH - (1-t)W_yH|| &= ||M -tW_xH -(1-t)W_yH||&\\leq ||tM-tW_xH|| + ||(1-t)M-(1-t)W_yH||\n",
    "\\end{align}$\n",
    "\n",
    "since $h(x) = x^2$ is convex we have\n",
    "$$ h(tx + (1-t)y)\\leq th(x) + (1-t)h(y) $$\n",
    "if  $x=z(x)$ \n",
    "$$ h(tz(x) + (1-t)z(y))\\leq th(z(x)) + (1-t)h(z(y))$$\n",
    "For $g(x) = h(z(x))$ to be convex, the inequality bellow must be true\n",
    "$$ h(z(tx + (1-t)y)) \\leq th(z(x)) + (1-t)h(z(y))$$\n",
    "\n",
    "since $z(W)\\geq0$, $h(z(W))$ is  monotonically increasing resulting in\n",
    "$$ h(z(tx + (1-t)y))\\leq h(tz(x) + (1-t)z(y)) \\leq th(z(x)) + (1-t)h(z(y))$$\n",
    "\n",
    "#### Gradient of G(W)\n",
    "The gradient of G(W) is the gradient of f(W,H), for $H=H^0$. So its gradient is $\\nabla_Wf(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "Write a function to compute g(W) and another to compute $\\nabla(W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g (W, H, M):\n",
    "    n, p = M.shape\n",
    "    w = np.matmul(W,H)\n",
    "    return 1/n/p * np.linalg.norm(M - w, ord = 'fro')**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_g (W,H):\n",
    "    n, p = M.shape\n",
    "    w = np.matmul(W,H)\n",
    "    return 1/n/p * np.matmul(w - M, H.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.4\n",
    "Let us define the function \n",
    "$$\\iota_{R_+}:\\mathbb{R}\\rightarrow \\mathbb{R} \\{+\\infty\\}$$\n",
    "$$x \\rightarrow \\left(\\begin{array}{ccc} 0, & if & x\\geq 0 \\\\ +\\infty, & if & x\\leq 0 &  \\end{array} \\right)$$\n",
    "Show that for all $\\gamma > 0$, $prox_{\\gamma \\iota \\mathbb{R}_+}$ is the projection onto $\\mathbb{R}_+$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "A projection is a linear tranformation P from a vector space to itself such that $P\\circ P=P$. And $prox_{\\gamma f}(x)=arg min_{y \\in \\mathbb{R}} f(y) + \\frac{1}{2\\gamma}*||y-x||^2$.\n",
    "\n",
    "<br>\n",
    "Given the function:\n",
    "\n",
    "$$\\iota \\mathbb{R}_+: \\mathbb{R} \\longrightarrow\\ \\mathbb{R}\\cup\\{+\\infty\\}$$\n",
    "$$x \\rightarrow \\left(\\begin{array}{ccc} 0, & if & x\\geq 0 \\\\ +\\infty, & if & x\\leq 0 &  \\end{array} \\right)$$\n",
    "\n",
    "and $\\gamma>0$, if $y<0$ than the $\\iota \\mathbb{R}_+(y) = +\\infty$, so we are going to make the calculation with $y\\geq0$.\n",
    "\n",
    "<br>\n",
    "The $arg min_{y \\in \\mathbb{R}} \\iota \\mathbb{R}_+(y) + \\frac{1}{2\\gamma}*||y-x||^2$ is y where:\n",
    "\n",
    "$$\\frac{\\delta \\iota \\mathbb{R}_+(y)}{\\delta y} + \\frac{\\delta g(y)}{\\delta y} = 0$$\n",
    "$$g(y) = \\frac{1}{2\\gamma}*||y-x||^2 $$\n",
    "\n",
    "for $y \\geq 0$ :\n",
    "\n",
    "$$\\frac{\\delta \\iota \\mathbb{R}_+(y)}{\\delta y} = 0 \\ \\ \\ \\ \\frac{\\delta g(y)}{\\delta y} = \\frac{y-x}{\\gamma} $$\n",
    "\n",
    "so we obtain:\n",
    "\n",
    "$$\\frac{y-x}{\\gamma} = 0$$\n",
    "\n",
    "Considering the case where $x\\geq0$, the argument minimum is $y=x$ and the $prox_{\\gamma \\iota \\mathbb{R}_+}(x)$ is equal to x. When $x<0$, $\\frac{\\delta g(y)}{\\delta y} > 0$ since $y\\geq 0$ and $\\gamma > 0$. This means that the value who minimizes the equation is the minimum in $y\\in [0,+\\infty [$, so $y=0$. Now we have:\n",
    "\n",
    "$$prox_{\\gamma \\iota \\mathbb{R}_+}(x) = \\left(\\begin{array}{ccc} x, & if & x\\geq 0 \\\\ 0, & if & x\\leq 0 &  \\end{array}\\right) $$\n",
    "\n",
    "By analyzing it properties, we see for $x\\geq0$:\n",
    "\n",
    "$$prox_{\\gamma \\iota \\mathbb{R}_+}(prox_{\\gamma \\iota \\mathbb{R}_+}(x)) = x = prox_{\\gamma \\iota \\mathbb{R}_+}(x)$$\n",
    "\n",
    "and for $x<0$:\n",
    "\n",
    "$$prox_{\\gamma \\iota \\mathbb{R}_+}(prox_{\\gamma \\iota \\mathbb{R}_+}(x)) = 0 = prox_{\\gamma \\iota \\mathbb{R}_+}(x)$$\n",
    "\n",
    "which is the definition of a projection, and $prox_{\\gamma \\iota \\mathbb{R}_+}(x) \\in \\mathbb{R}_+$, that means $prox_{\\gamma \\iota \\mathbb{R}_+}$ is the projection onto $\\mathbb{R}_+$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.5\n",
    "Code a function projected gradient method(val g, grad g, W0, gamma, N) that minimizes a function g subject to nonnegativity constraints by the projected gradient method\n",
    "with a constant step size, starting from W0 and stopping after N iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The gradient method concist in computing\n",
    "$$x_{k+1} = x_k - \\gamma_k\\nabla f(x_k)$$\n",
    "until a stop criteria is met. The step size $\\gamma_k$ can be a chosen in a variety of ways but, for the first implementation, it will be considered constant.\n",
    "Since the only constraints for the given problem is the nonnegativity, all the negative solutions will be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method (grad, W, gamma, N):\n",
    "    # grad   =    function to be used for the gradient computation\n",
    "    # W      =    Initial position for the gradient method\n",
    "    # gamma  =    step size for the next point\n",
    "    # N      =    number of iterations\n",
    "    \n",
    "    \n",
    "    # w1     =    value of the next point\n",
    "    \n",
    "    w = W\n",
    "    for n in range (0,N):\n",
    "        w1 = w - gamma*grad(w,H0)\n",
    "        # numpy.clip(), limits the input data passed trhoug as the first argument to the interval a_min-a_max.\n",
    "        # if the value of the input is smaller than a_min, it is set to a_min, if gratter the a_max, it is set to a_max;\n",
    "        # set one boundary to None, to not limit the value \n",
    "        w = np.clip(w1, a_min=0, a_max=None)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.6\n",
    "Use the function to minimize g with N = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Since the stepsize is being considered a constant, it will be set to $\\frac{1}{L_0}$, where $L_0$ is the Lipschtz constant,\n",
    "$$L_0 = ||(H^0)^TH^0||_F$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[2.08038888e-02 2.19321069e-01 1.18960814e-01 ... 1.14058759e+01\n",
      "  6.61448859e-05 0.00000000e+00]\n",
      " [2.89872220e-02 8.63262065e-06 3.40224096e-01 ... 5.45957801e-05\n",
      "  6.27851936e-05 0.00000000e+00]\n",
      " [3.99948449e-01 8.32476644e-06 1.36229564e-01 ... 5.58385661e+00\n",
      "  6.29376307e-05 0.00000000e+00]\n",
      " ...\n",
      " [1.30190138e+00 4.08582328e-06 7.64392818e-01 ... 1.82621864e-05\n",
      "  8.53313948e-02 0.00000000e+00]\n",
      " [8.54284570e-06 2.56440478e-01 3.23851824e-03 ... 4.57719862e-05\n",
      "  7.41929775e-05 0.00000000e+00]\n",
      " [7.25785035e-06 5.07637986e-01 7.57318785e-06 ... 3.15387163e-05\n",
      "  6.82531351e-05 0.00000000e+00]] \n",
      " g(W): 3959.910882541806\n"
     ]
    }
   ],
   "source": [
    "L0 = np.linalg.norm(np.matmul(H0.T, H0), ord = 'fro')\n",
    "gamma = 1/L0\n",
    "W_static = projected_gradient_method(grad_g, W0, gamma,100)\n",
    "print(\"W:\", W_static, \"\\n\",\"g(W):\", g(W_static, H0, M) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Algorithmic refinement for the problem with H0 fixed\n",
    "#### Question 4.1\n",
    "Implement a line search to the projected gradient method, in order to free ourselves from\n",
    "the need of a known Lipschitz constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To implement a line search we need to sarch for a step size that respect the following equations\n",
    "$$x^+(\\gamma_k) = x_k - \\gamma_k\\nabla f(x_k)$$\n",
    "$$\\gamma_k = ba^l\\text{    ;    } b>0\\text{    ;    }  a \\in (0,1)$$\n",
    "$$f(x^+(ba^l))\\leq f(x_k) + \\left<\\nabla f(x_k),x^+(ba^l)-x_k\\right> + \\frac{1}{2ba^l}||x_k-x^+(ba^l)||^2 $$\n",
    "The initial value of b and a is set to 1 and 0.5. After the first iteration, the value of b is set to double the last step. To search for the best $x^+(ba^l)$, the inequality is tested for integers values of $l$ starting at 1. The smallest $l$ that turns the inequality true, will give the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method_linear_search (func, grad, Var, cte, N):\n",
    "    # Var    =    initial position for the gradient method\n",
    "    # cte    =    position considered constant for the gradient method\n",
    "    # grad   =    function to be used for the gradient computation\n",
    "    #             If Var=W, grad should be grad_g, if Var=H, grad should be grad_h\n",
    "    # func   =    function to be used for the computation of g(W,H)\n",
    "    #             If Var=W, func should be g, if Var=H, func should be g_h\n",
    "    # N      =    number of iterations\n",
    "    \n",
    "    \n",
    "    # b,a    =    parameters for the linear search\n",
    "    # bool   =    boolean variable to test if the next value for step has been found\n",
    "    # gamma  =    value of the step\n",
    "    # inner_product = compute the value of the innerproduct present at the inequality\n",
    "    # norm   =    compute the value of the norm present at the inequality\n",
    "    \n",
    "    b, a = 1, 0.5\n",
    "    var = Var\n",
    "    \n",
    "    # loop for the n steps to be taken\n",
    "    for i in range (0, N):\n",
    "        bool, l = True, 1\n",
    "        # loop for the search of the stepsize \n",
    "        while bool:\n",
    "            # test possible step size\n",
    "            gamma = b * a**l\n",
    "            var1 = var - gamma*grad(var, cte)\n",
    "            var1 = np.clip(var1, a_min=0, a_max=None)\n",
    "            \n",
    "            inner_product = np.matrix.trace( np.matmul(grad(var, cte), (var1 - var).T ) )\n",
    "            norm = 1/(2*b*a**l) * np.linalg.norm(var - var1, ord = 'fro')**2\n",
    "            bool = func(var1, cte, M) < func(var, cte, M) + inner_product + norm\n",
    "            # if bool = True then the step size was found and one iteration is complete\n",
    "            if (bool):\n",
    "                var = var1\n",
    "                b = 2*gamma\n",
    "                bool = False\n",
    "            else:\n",
    "                l += 1\n",
    "    return var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 0.30792192  0.45459135  0.38440341 ... 14.05888182  2.96308729\n",
      "   0.        ]\n",
      " [ 0.34272309  0.309057    0.67240608 ...  2.44701943  2.40628859\n",
      "   0.        ]\n",
      " [ 0.72017863  0.2845636   0.46018469 ...  8.48771006  2.4049391\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.45542915  0.14206547  0.94397485 ...  0.54677866  2.69140178\n",
      "   0.        ]\n",
      " [ 0.30964053  0.55471907  0.32834924 ...  1.80386281  3.33482435\n",
      "   0.        ]\n",
      " [ 0.26601008  0.77829979  0.28978753 ...  0.96576292  3.25269593\n",
      "   0.        ]] \n",
      " g(W): 2492.8308823803063\n"
     ]
    }
   ],
   "source": [
    "W_linear_search = projected_gradient_method_linear_search(Var=W0, cte=H0, func=g, grad=grad_g, N=100)\n",
    "print( \"W:\", W_linear_search, \"\\n\",\"g(W):\", g(W_linear_search, H0, M) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "Compare the performance of both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The linear search algorith achives a better aproximation in the same number of iterations. For this reason it is a better aproach to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Resolution of the full problem\n",
    "#### Question 5.1\n",
    "Solve Problem (1) by the projected gradient method with line search for N = 1000 iterations.\n",
    "What does the algorithm return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 0.35270143  0.10060651  0.24090835 ... 22.67437113  8.80012211\n",
      "   0.        ]\n",
      " [ 0.36761635  0.3784766   0.88806493 ...  8.56296598  3.02156043\n",
      "   0.        ]\n",
      " [ 0.66843361  0.          0.44267527 ... 17.10893915  2.83593457\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.3855232   0.          1.07331059 ...  0.         11.96498542\n",
      "   0.        ]\n",
      " [ 0.23262002  0.40605176  0.36994483 ...  3.06091683 10.38343371\n",
      "   0.        ]\n",
      " [ 0.2303479   0.81457935  0.44320564 ...  0.         11.50426724\n",
      "   0.        ]] \n",
      " g(W): 2086.7501430792067\n"
     ]
    }
   ],
   "source": [
    "W_linear_search_1000 = projected_gradient_method_linear_search(Var=W0, cte=H0, func=g, grad=grad_g, N=1000)\n",
    "print (\"W:\", W_linear_search_1000, \"\\n\",\"g(W):\", g(W_linear_search_1000, H0, M) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.2\n",
    "When W (resp. H) is fixed, the problem is easier to solve. The method of alternate minimizations uses this fact and consists in the following method:\n",
    "***\n",
    "for $t \\geq 1$ do<br>\n",
    "    $W_t \\leftarrow arg min_W \\frac{1}{2np}||M - WH_{t-1}||^2_f $<br>\n",
    "    $H_t \\leftarrow arg min_H \\frac{1}{2np}||M - W_tH||^2_f $<br>\n",
    "end for\n",
    "***\n",
    "Show that the value of the objective in decreasing at each iteration. Deduce from this\n",
    "that the value converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "On each iteration of the algorithm we compute:\n",
    "\n",
    "$$W_t = argmin_W \\frac{1}{2np}*||M-WH_{t-1}||_F^2$$ by fixing H, and\n",
    "\n",
    "$$H_t = argmin_H \\frac{1}{2np}*||M-W_tH||_F^2$$\n",
    "\n",
    "by fixing W. We saw before that by fixing W or H, the objective function becomes convex, which guarantees the existence and uniqueness of the minimum argument. We know that:\n",
    "\n",
    "$$f_M(W,H_0) \\geq f_M(W',H_0)$$ where\n",
    "$$W' = argmin_W \\frac{1}{2np}*||M-WH_0||_F^2$$\n",
    "and\n",
    "$$f_M(W_0,H) \\geq f_M(W_0,H')$$ where\n",
    "$$H' = argmin_H \\frac{1}{2np}*||M-W_0H||_F^2$$\n",
    "which leads to\n",
    "$$f_M(W_t,H_t)\\leq f_M(W_t,H_{t-1})\\leq f_M(W_{t-1},H_{t-1})$$\n",
    "\n",
    "If the W matrix and/or H matrix changes, the new value of $f_M(W_t,H_t)$ is lesser than it value of the last iteration, and:\n",
    "\n",
    "$$(W^*,H^*)\\in argmin_{(W\\geq 0,H\\geq 0)} \\frac{1}{2np}*||M-WH||_F^2$$ where\n",
    "$$ f_M(W^*,H^*)\\leq f_M(W_t,H_t)$$ for all $t\\geq 1$.\n",
    "\n",
    "So if $f_M(W_t,H_t)$ continues to decrease, than it converges to $f_M(W^*,H^*)$ or to a local minimum. The algorithm always converge due to it non increasing behavior and to having an inferior limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.3\n",
    "Code the alternate minimizations method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_h(var, cte, M):\n",
    "    # give the correct arguments to the function g(W,X) for when the variable is H, and not W\n",
    "    return g(cte, var, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_h (var,cte):\n",
    "    n, p = M.shape\n",
    "    w = np.matmul(cte,var)\n",
    "    return 1/n/p * np.matmul(cte.T,w - M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_minimazation (W0, H0, N):\n",
    "    # W0    =    initial position for W to be given to the gradient method\n",
    "    # H0    =    initial position for H to be given to the gradient method\n",
    "    # N     =    number of iterations\n",
    "    \n",
    "    # computes initially a new value for W, considering H as cte. Then computes a new value for H, considering the new \n",
    "    # W as cte.\n",
    "    \n",
    "    W, H = W0, H0\n",
    "    for i in range (0,N):\n",
    "        W1 = projected_gradient_method_linear_search(g, grad_g, W, H, 1)\n",
    "        H1 = projected_gradient_method_linear_search(g_h, grad_h, H, W1, 1)\n",
    "        W, H = W1, H1\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 0.19687229  0.36079898  0.27322103 ... 13.35465546  1.92750811\n",
      "   0.        ]\n",
      " [ 0.21955056  0.20117862  0.54789584 ...  1.5326444   1.08159605\n",
      "   0.        ]\n",
      " [ 0.61038736  0.19005301  0.34817373 ...  7.79626124  1.36230673\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.34998707  0.05577374  0.84328243 ...  0.          1.68723653\n",
      "   0.        ]\n",
      " [ 0.20280569  0.46659054  0.224336   ...  1.09384235  2.34546422\n",
      "   0.        ]\n",
      " [ 0.17314086  0.70331957  0.20022561 ...  0.30281551  2.41870912\n",
      "   0.        ]] \n",
      " H: [[0.16277783 0.03479984 0.01750423 ... 0.         0.         0.33980119]\n",
      " [0.18039911 0.03373745 0.16038512 ... 0.         0.24922383 0.        ]\n",
      " [0.07494868 0.0338743  0.12345815 ... 0.         0.62638586 0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.45384567 2.46264167 2.46702801 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] \n",
      " f(W,H): 845.9286814637501\n"
     ]
    }
   ],
   "source": [
    "w_alternate, h_alternate = alternate_minimazation(W0=W0, H0=H0, N=100)\n",
    "print ( \"W:\", w_alternate, \"\\n\", \"H:\", h_alternate, \"\\n\", \"f(W,H):\", g(w_alternate, h_alternate, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.4\n",
    "Compare projected gradient and alternate minimizations methods. Are the solutions the same? Is the objective value the same? How do the computing times compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The alternate methode is way better than the linear search. For the same number of steps the alternate method was able to obtein a pair W, H that gave $f(W,H) = 845.9286814637501$, while the linear search was only able to reduce $f(W,H) = 2492.8308823803063$. Even with 1000 iterations, the linear search was not able to beat the alternate method, obtaining $f(W,H) = 2086.7501430792067$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.5\n",
    "What stopping criterion could be used for the algorithms instead of just the number of\n",
    "iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Two other criteria come to mind:\n",
    "*  Precision - The algorithm stops only after f(W, H) was smaller than a determined value. One problem of this implementation is that the computing time is not clearly established. \n",
    "* Computation time, instead of using the number of iterations, a time limit for the operation could be set, returning the smaller value obtained during that time;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
